<!DOCTYPE html>
<html>
	<head>
		<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
		<link href='https://fonts.googleapis.com/css?family=Merriweather:400,300' rel='stylesheet' type='text/css'>
		<link href="../globalstyle.css" rel="stylesheet" type="text/css">

        <!-- Javascript below adapted from https://smallbusiness.chron.com/expand-collapse-text-47345.html and https://www.ehow.com/how_5772308_expand-collapse-text.html -->
		<script language="JavaScript" type="text/javascript">
		function ExpandCollapse(theDiv,theButton){
			el = document.getElementById(theDiv);
			button = document.getElementById(theButton)
			if(el.style.display == 'none')
			{ el.style.display = '';
			  button.innerHTML="(Click here to hide input text.)";}
			else
			{ el.style.display = 'none';
			  button.innerHTML="(Click here to show input text.)";}
			return false; } </script>


		<title>
			John Loeber: Television
		</title>
	</head>

	<body>
		<div style="width:751px;height:20px;background-color:#FFFFFF"></div>
			<div class="siteimg"></div>
			<div class="siteheader"> John Loeber </div>
		<div style="height:10px;width:490px"></div>

		<div style="width:167px;height:120px;float:left">
			<a href="../index.html"><div class="block" id="linkblock"><span class="blockfont">Projects</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<div style="width:167px;height:120px;float:left">
			<a href="../writing.html"><div class="block" id="linkblock"><span class="blockfont">Writing</span></div></a>
			<div style="width:10px;height:120px;float:right"></div>
		</div>

		<a href="../about.html"><div class="block" id="linkblock"><span class="blockfont">About</span></div></a>

	<div class="main">
		<br>
	<atitle>Television 1950-2014 Episode Summary Dataset, with Analysis</atitle></td>
	<br><br>
	<sh>Abstract</sh>
	<p>Wikipedia contains a large number of episode summaries of television shows that aired in the USA during the period 1950-2014.
	Scraping Wikipedia for this data yields in a powerful dataset of word frequencies by year, totaling over four million datapoints.
	Assuming a close relationship between a society and its televised content, analysis of this dataset enables one to make certain sociological
	observations. This article presents the dataset, related analytical tools, as well as some informal results of exemplary analysis.</p>
	<br>
	<sh>Introduction</sh>
	<p>This project contains several different parts:
	<ul>
	<li><i>Frequencies</i> </li>
	<li><i>Whole-Episodes</i></li>
	<li><i>Selected-Shows</i></li>
	</ul>
		These three tools are all implemented similarly, but the resulting datasets differ. The <i>frequencies</i> and <i>whole-episodes</i> datasets represent,
		in different ways, all Wikipedia episode summaries of television shows that aired between 1950 and 2014 in the USA. The <i>selected shows</i>
		tool differs in that it requires the user to submit a list of shows for which to scrape the episode summaries. All datasets
		included are current as of November 1, 2014.
	</p>
	<p>
	All of these episode summaries were written quite recently, since Wikipedia is only 13 years old. The benefit of this is that it is reasonable to assume
	that these episode summaries generally use a homogeneous corpus of modern language, so that it is possible to meaningfully compare these texts by year.
	The purpose of making comparisons between episode summary texts is to drive sociological or anthropological conclusions. This is possible, based on a chain of
	two assumptions: that an episode summary actually corresponds closely to the content of the televised episode, and that general televised content macroscopically corresponds
	sufficiently closely to that society's sentiments to change with a change in sentiment. Where the causality lies in that correspondence is unknown (does television reflect a society's interests, or does television
	define a society's interests?) and beyond the scope of this article, though it is an interesting question to pursue.
	</p>
	<p>
	Of course there are limitations to these datasets. It may be that the editors of these Wikipedia articles represent some specific subset of society, whereby
    the articles may have some systematic bias toward particular themes or toward particular linguistic expressions. Additionally, the modern shows are generally
    better-represented than older ones:
    <br>
	<img class="imgcenter" src="../images/wordfreqsbyyear.png"></img>
    <br>
    This is both a blessing and a curse, because (it seems safe to assume that) the implicit effect is that only a particular subset of older television shows have had their episodes
    summarized in significant detail on Wikipedia: specifically, it is likely that the episode summaries of the 1950s through 1980s are generally of the more remarkable or memorable
    (i.e. culturally significant) television shows, whereas for more recent television shows, even the ones that aren't of great cultural significance are documented quite thoroughly. All episode summaries are currently
    weighted evenly, which yields a bias in favor of the content of the less popular shows (naturally, the most popular/watched shows are a tiny fraction of the total number of shows on air at any given point in time).
    It is not known what the relation between less popular and more popular television shows is in the extent to which they represent general societal sentiment. One would assume that the more popular shows better represent general societal sentiment, but this may not necessarily be the case. This uncertainty, of course, significantly reduces the reliability of the results. Nonetheless, I hope that the collected dataset and tools allow for some useful investigations, or at the least, good amusement. Overall, I hope that these dataset and tools
	will allow for investigations the way <a href="https://books.google.com/ngrams">Google Books' ngram viewer</a> does.
	</p>
<br>
<sh>Results </sh>
<p>The following are some exemplary results that follow from analyses of the datasets I collated with the tools I made. Note that these results do not exhaust (at all) the potential of these tools and datasets. </p>
<p>
The big question that fundamentally drove this investigation was &ldquo;has society
become more sexualized in the last sixty years?&rdquo; Such a claim is frequently
thrown around, but is rarely backed up with quantitative evidence. Here's an analysis
using <i>Frequencies</i> and <a href="../files/sexualdictionary.txt">this</a> dictionary:

	<img class="imgcenter" src="../images/televisionsexindex.png"></img>
It appears that the small number of samples (see diagram in <i>Introduction</i>) before approx. 1975
causes significant spread among the datapoints, such that this data may not be usable. This is unfortunate. I removed an outlier at 1950.
<br><br>
From about 1975 onwards, we can discern a wavelike pattern in the datapoints: perhaps
this reflects some sort of sociopolitical pattern of liberal/conservative reactions:
a sexually liberal push, then a conservative pull back, etc. This wavelike pattern
could be a consequence of any number of plausible reasons.
Humorously, I note that the most consistent peak in sexual index seems to have
occurred in the 90s, followed by a severe slump. Could it have been caused by a taboo following the Lewinsky scandal?
Could it be a consequence of post-9/11 sorrow and a changed sociopolitical climate?
</p>
<p>Thus, the next <i>Frequencies</i>-type analysis uses <a href="../files/happydict.txt">this</a> dictionary, words commonly
associated with happiness, and attempts to track a &lsquo;happiness index&rdquo;:

	<img class="imgcenter" src="../images/tvhappy1.png"></img>
The data before 1965 seems too subject to random error, so we zoom in on 1965 onwards:
	<img class="imgcenter" src="../images/tvhappy2.png"></img>
Notice how commensurate the Happiness and Sexuality indices are. Both share a slump to an all-time low
at around 1985, and the same slump between 2000 and 2010. Indeed, these datapoints have a correlation of 0.695. That's strong.
(I used MATLAB's <code>corr</code> function on the two sets of datapoints to obtain this result.) I then was concerned,
because both dictionaries contained the common, strongly-weighted word <i>love</i>: could that account for the strength of this correlation? I removed
<i>love</i> from both dictionaries, and ran the computations again. Now these two datasets have a correlation of 0.572, which is still rather strong in this context. (MATLAB reported p-values of 0 for both correlations.) This correlation is a very interesting result &mdash; unfortunately, the question of causality is beyond the scope of this article.
</p>
<p>
During this time period, we've also seen the advent of gay rights, and more recently,
greater social equality for LGTBQ persons. This should be reflected clearly by televised content.
Another <i>Frequencies</i>-type analysis using <a href="../files/lgbtqdict.txt">this</a> dictionary
attempts to track LGBTQ discussion on television:
	<img class="imgcenter" src="../images/tvlgbtq.png"></img>
This looks more or less like what you would expect. Note the low datapoints in the mid-1980s: I assume that this reflects the HIV/AIDS scare. This movement is commensurate with the trough in the Television Sexuality Index, above.
</p>
<p>
Another big question concerns the focus on violence in this society. Virtually all statistics indicate that the
world is continuously becoming a safer place. However, many people are under the (false) impression that the world is
continuously becoming more dangerous. Let's see how this is reflected by television. This is yet another <i>Frequencies</i>-type
analysis using <a href="../files/dangerdict.txt">this</a> dictionary:
	<img class="imgcenter" src="../images/tviolence.png"></img>
I was surprised by the mountainous peak in the 1960/70s. Could this be due to the Vietnam War and the Cold War?
I am aware that the that Vietnam War has a reputation as being the &ldquo;first televised war,&rdquo; but beyond that,
I'd be very curious to find out the sociological/cultural underpinnings of this trend.
</p>
<i>Technical note: for all of these results, stopwords were filtered out. Also, if you disagree with my methodology, i.e. word-weights, feel free to change up the weights and run the scripts yourself! I'd like to see what you come up with.
From here on, this article is mostly a technical documentation of and usage guide for the datasets and tools that I used to find the above results, and more.</i>
<br><br>
	<br>
    <sh>Frequencies: Documentation</sh>
	<p>
	The script <code>getfrequencies.py</code> scrapes Wikipedia for all episode summaries, from which it computes word frequencies by year. The script <code>postprocess.py</code>
    needs to be run afterwards to clean up the data. Then it is ready for analysis. The tool <code>analysis.py</code> can assist with the more obvious types of analyses. The dataset
    itself is a set of files labeled by year: &ldquo;1950&rdquo; through &ldquo;2014&rdquo;. Each file contains three lines. The first line is the dictionary of word frequencies. The
    second line is the total word count of that dictionary. The third line is the number of unique words in the dictionary. To give an example, this is what a dataset might look like:
    <p style="margin-left:4em;margin-right:4em;">
	<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
	{'she': 100, 'wish': 1, 'vies': 1,'official': 1, 'accuses': 3, 'giranins': 1, 'acquired': 3}<br>110<br>7
	</span>
	</p>

    The dataset has two variations: <i>with-stopwords</i>, which doesn't filter out any words, and <i>without-stopwords</i>, which has all <a href="https://en.wikipedia.org/wiki/Stop_words">stopwords</a> filtered out. The datasets themselves are located in the <code>.zip</code> folder labeled <code>DATASET1</code>.

	</p>
    <br>
    <sh>Frequencies: Implementation Details</sh>
<p>
    <i>If you just want to work with the datasets, then you don't need to read this section.</i> <br><br> <code>getfrequencies.py</code> is written using Python 2.7.6 and using the <code>collections</code>,
	<code>bs4</code> (BeautifulSoup), and <code>urllib2</code> libraries. It was written for use on a UNIX system, though it probably
	works on Windows, too (untested). Here's how it works:
	<ol>
	<li>It starts at <a href="https://en.wikipedia.org/wiki/Category:Television_seasons_by_year">Wikipedia's category of television seasons
	by year</a>. This page contains links to subcategories of television seasons for given years, such as
	<a href="https://en.wikipedia.org/wiki/Category:2002_television_seasons">2002 television seasons</a>,
	<a href="https://en.wikipedia.org/wiki/Category:1954_television_seasons">1954 television seasons</a>,
	etc. It finds the links to all of these subcategories: one for each year.</li>
	<br>
	<li>Each of these subcategories lists and links to articles on all television show seasons that
	aired in a subcategory's respective year. The script finds all these links, and follows them. For
	example: the category <a href="https://en.wikipedia.org/wiki/Category:1990_television_seasons">1990
	television seasons</a> contains a link to <a href="https://en.wikipedia.org/wiki/The_Fresh_Prince_of_Bel-Air_(season_1)">The Fresh Prince of Bel-Air (Season 1)</a>.</li>
	<br>
	<li>Each of these <i>Television Show (Season n)</i> pages contains a <i>List of Episodes</i>, in which each episode is summarized. The script finds the list of episodes, parses the text
        (removing all punctuation and other detritus),
        and adds word occurrences to that year's dictionary of word frequencies.</li>
	<br>
	<li>The dictionary of word frequencies for each year is output as a file labeled by year, as seen above.</li>
	</ol>
	The code itself is really straightforward. <code>getfrequencies.py</code> only takes one command-line argument: <code>y</code> to filter, or <code>n</code> to not filter stopwords.
    Thus, it can be run at the command-line with <code>python getfrequencies.py y</code> or <code>python getfrequencies.py n</code>. When filtering stopwords, it draws on a file that is a list
    of stopwords, <code>stopwords-final</code>. This list comprises two lists of stopwords found <a href="https://norm.al/2009/04/14/list-of-english-stop-words/">here</a> and <a href="https://www.ranks.nl/stopwords/">here</a>. </p>

	<p>As aforementioned, the output of <code>getfrequencies.py</code> is a set of files, &ldquo;1950&rdquo; through &ldquo;2014&rdquo;. However, these files tend to have
    some issues with Unicode representation. Essentially, some HTML ascii, like <code>&amp;nbsp;</code> is parsed into
	Unicode by BeautifulSoup, yielding the non-ascii character <code>\xc2\xa0</code>. Thus, some dictionary entries, like this one, are garbled: <code>'fianc\xc3\xa9e': 1</code>.</p>
	<p>Thankfully, this can be corrected (i.e. these unicode representations can be replaced with regular ascii characters). The script
	<code>postprocess.py</code> has been written, in Python 2.7.6, for this purpose. It requires the <code>sys</code>, <code>os</code>, <code>ast</code>, and <code>collections</code> libraries.
    </p>
    <p>
    <code>postprocess.py</code> takes a directory of <code>getfrequencies.py</code>-output files as a command line argument. For example, suppose I use <code>getfrequencies.py</code> to generate files
	&ldquo;1950&rdquo; through &ldquo;2014&rdquo;, and I put all these

	into the directory <code>without-stopwords/</code>. Then I will call <code>python postprocess.py without-stopwords/</code>. The script will create a new directory,
	<code>2without-stopwords/</code>, containing the correctly processed files. They may still contain negligible amounts of garbled output, as the postprocessing script does
	not filter out <i>all</i> unicode representations. An additional
	 script, <code>gettotal.py</code>, has been included for testing
	the output. <code>gettotal.py</code> also takes a directory as a command-line argument, and reports back various statistics, and writes a file <code>
	nonascii</code> that contains a dictionary containing all non-ascii (i.e. garbled) words. In my experience, there are is so little garbled output that it can be safely discarded.
	Thus, continuing our example, run <code>python gettotal.py 2without-stopwords/</code>
	in order to assess the quality of the processed output.</p>
	<p>
	The post-processed files are in the same format as the pre-processed ones. Thus, each year's file contains a dictionary of word frequency counts for that year on the first line, the total number of words on the second line, and the number of unique words on the third line.
	This dictionary can be read trivially into python using <a href="https://stackoverflow.com/q/988228/2844655">ast.literal_eval</a>. Note that the format is quite similar to
	JSON, so parsing this data is easy in many languages.
	</p>

<p>Note that these scripts have not been optimized for efficiency &mdash; running the script to collect word frequencies for 1950 through 2014 may take a few minutes. I think that the workflow
could generally be substantially improved and simplified. Unfortunately, I currently cannot justify spending further time on these optimizations.</p>

	<br>
	<sh>Frequencies: Analysis</sh>
	<p>We can now use the postprocessed data for sociological/cultural analysis. There are many ways to analyze this dataset, for one of which I've built a little tool.</p>
    <p>
    As the data is a set of word-frequency dictionaries by year, the obvious course of attack
    is to track the changes in frequency for some word that we are interested in, over time. We observed previously that the total wordcounts generally rise over time,
    so it is not wise to track raw frequency changes, but rather
    proportion: the fraction of the total wordcount that is the word(s) we are interested in. On top of that, we can <i>weigh</i> particular words more than others when considering
    this proportion. For example, suppose we are interested in the popularity of physics on television over time. We'll give &ldquo;physics&rdquo; a weight of 1, &ldquo;quantum&rdquo; a weight of 0.9 (implying that we think that 90% of uses of the word &ldquo;quantum&rdquo; are in the context of physics), the word &ldquo;gravity&rdquo; a weight of 0.5, the word &ldquo;molecule&rdquo; a weight of 0.9, etc. We can then find the weighted proportion per year, and compare that figure over the course of time. This is what the tool <code>analysis.py</code> does. </p>
    <p>
    <code>analysis.py</code> takes two command line arguments: the first is a dictionary of space-separated weights and words, the second is a directory of postprocessed files (I'll use the ones without stopwords in this example, as stopwords are just semantic clutter in this context). For example, let's suppose the dictionary is called <code>worddict</code>, and these are its contents:
    <p style="margin-left:4em;margin-right:4em;">
	<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
	1.0 physics<br>
    0.9 quantum<br>
    0.5 gravity<br>
    0.9 molecule<br>
    0.9 atom<br>
    0.25 science<br>
    1.0 physicist<br>
    0.75 nasa<br>
    </span>
	</p>
    Then I can call <code>python analysis.py worddict 2without-stopwords/</code>, and it will output a file, <code>dictionary_2withoutstopwords</code>. This file will contain summative data on every year. This data is formatted as in this example:
 <p style="margin-left:4em;margin-right:4em;">
	<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
{'science': 4.25, 'gravity': 1.0, 'physicist': 3.0, 'nasa': 3.0, 'atom': 3.6, 'quantum': 0.9, 'physics': 4.0}<br>
2007 19.75<br>
0.000209074356369<br>
</span>
</p>

The format is always the same: on the first line, there is a dictionary that displays the cumulative weights of all words. Note that in our example, this line is wrapped onto two lines. The second line contains the year and the total weight. The third line displays the proportional representation of the dictionary that year: that's the total weight of our dictionary-words (19.75) divided by the total number of words in the 2007 dictionary. I think that third line is the really important statistic to track and compare over time.
Consequently, I also wrote the script <code>dictoresults.py</code> to format this output. Running <code>python dictoresults.py worddict_2without-stopwords</code> will produce <code>results_worddict_2without-stopwords</code>, which is just a file of space-and-comma-separated years and proportional representations, like &ldquo;2007, 0.000209074356369&rdquo; which is easier to work with.
</p>
<p>However, note the limitations of this statistic: the word-counts are usually very small. For example, in 2007, the word &ldquo;physicist&rdquo; appears three times. This makes it possible for outliers to significantly skew the results. Thus, it is probably important to use large dictionaries with wisely-chosen weights in order to arrive at more reliable results.
</p>
<p>
One important configuration that you must choose depending on your dictionary is
on line 31 of <code>analysis.py</code>, which gives you the option of matching
words in the text to words in your dictionary either by <i>substring</i> or by
<i>string equality</i>. This is important because some words are often contained
in other words, e.g. &ldquo;eat&rdquo; is contained in &ldquo;eating,&ldquo;
and &ldquo;eaten,&rdquo; but also in &ldquo;beat,&rdquo; &ldquo;neat,&rdquo; etc.

</p>
<br>
<sh> Whole-Episodes: Documentation</sh>
<p>
This is very straight-forward. A script called <code>whole-episodes.py</code> scrapes Wikipedia for all episode summaries. It filters out punctuation and other clutter, and then writes the plain text of the episode summaries to a textfile, separating different episodes with newlines, and separating different shows with double newlines. This script doesn't take any command-line arguments and can thus be run just with <code>python whole-episodes.py</code>. Like the scripts in <i>Frequencies</i>, it relies on the <code>bs4</code>, <code>sys</code>, <code>collections</code> and <code>urllib2</code> libraries. The advantage of having the plaintext episode summaries is that it allows one to do more sophisticated textual analysis, using tokenization, phrasal search, etc. This dataset is in a <code>.zip</code> folder labeled <code>DATASET2</code>.
</p>
<br>
<sh> Selected-Shows: Documentation</sh>
<p>
This differs from <i>Whole-Episodes</i> only in that (1) it requires the user to select a particular television shows to gather data on, and (2) it runs comparisons on an episodal, rather than on an annual basis. The script to retrieve episode summaries from Wikipedia is <code>retrieve.py</code>. This yields a set of documents (though in some cases, just one) containing episode summaries of the television show, separated by <code>&lt;new episode&gt;</code>. The script <code>text-analysis.py</code> then works similarly to the analysis in <i>Frequencies</i>: it takes a dictionary with weights, reads in every episode, and then computes the weighted representation of the dictionary terms for that episode as a fraction of the number of words in that episode, discounting stopwords. If you wish to not filter out stopwords, just comment out lines 36 through 38, and outdent line 39.
</p>
<br>
<sh> Selected-Shows: Usage Guide</sh>
<p>
First off, you need a television show you want to investigate. Go to Wikipedia and find that television show's list of episodes. Usually these pages are titled <i>Television Show (Season n)</i>. Once you have found these pages, copy-paste the links into a notepad file. I'll use <i>Seinfeld</i> as an example. For Seinfeld, your document of urls (we'll call it <code>seinfurls</code>) should look like this:
<p style="margin-left:4em;margin-right:4em;">
<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
https://en.wikipedia.org/wiki/Seinfeld_%28season_1%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_2%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_3%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_4%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_5%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_6%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_7%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_8%29
https://en.wikipedia.org/wiki/Seinfeld_%28season_9%29
</span>
</p>
These are, in order, the links to the first through ninth seasons of Seinfeld. Now you can run <code>python retrieve.py seinfurls</code>. It will return a set of files, <code>souped_0</code> through <code>souped_8</code>. Here, <code>souped_0</code> corresponds to the page for the first season of Seinfeld, <code>souped_1</code> corresponds to the second season, etc. and <code>souped_8</code> corresponds to the ninth season of Seinfeld. Each document is just a list of episodes separated by some newlines and the text <code>&lt;new episode&gt;</code>. Now, place all your <code>souped</code> files into a directory, like <code>seinfeld-episodes/</code>. Now you have a dataset. Because you have the plaintext of the episodes without words filtered, you can conduct whatever Natural Language Processing-type analysis you like. However, I have prepared a small script for the obvious frequency-type analysis.
<br>
<br>
This small script is <code>text-analysis.py</code>. It takes three command line arguments: the first is a directory of sequentially numbered files that contain
the episode summaries to be analyzed. The second argument is a dictionary-file of space-separated weights and words, as discussed in the <i>Frequencies</i> section.
The third argument is just a <code>y</code> or <code>n</code> depending on whether or not you want stopwords to be filtered out (<code>y</code> to filter). Note that
if you select <code>y</code>, then you need the file <code>stopwords-final</code> in your working directory.
</p>
<p>
To continue our example: Seinfeld is famously a show about nothing, but I don't really believe that, so let's investigate the importance of <i>personal relationships</i> in Seinfeld.
So we write this dictionary and save it as <code>dictionary</code>:
<p style="margin-left:4em;margin-right:4em;">
<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
0.75 friend<br>
1.0 relationship<br>
0.75 date<br>
1.0 dating<br>
0.5 sex<br>
0.5 love<br>
</span>
</p>
My choice of weights for these words reflects how closely I consider them to be associated with relationships. Then we run <code>python text-analysis.py seinfeld-episodes/ dictionary y</code>. It will output a file called <code>seinfeld-analyzed</code>. This outputs a file that looks like this (excerpt):
<p style="margin-left:4em;margin-right:4em;">
<span style="display:inline-block; width:100%; text-align:left;background-color:#E8E8E8">
0.75, 12, 0.0625<br>
0.75, 23, 0.0326086956522<br>
0.75, 35, 0.0214285714286<br>
1.5, 27, 0.0555555555556<br>
<br>
<br>
0, 28, 0.0<br>
0.75, 25, 0.03<br>
0.75, 34, 0.0220588235294<br>
1.25, 20, 0.0625<br>
</span>
</p>
In this context, a double newline (space between blocks of lines) means that the script is handling the next sequentially numbered document in the directory. A newline or line-break indicates a different episode. The three comma-and-space-separated numbers on every line indicate (1) weighted representation of dictionary words in that episode, (2) total words in that episode summary, and (3) the ratio of (1) to (2). So let's plot (3) over the course of Seinfeld's episodes. How you plot this is up to you, but I wrote the <code>matlab-analysis</code> script to do it:

	<img class="imgcenter" src="../images/seinfeld.png"></img>
</p>
<p>
The red regression line shows an upward trend, suggesting that the importance of personal relationships actually grew during the run of Seinfeld. Note, however, that the size of this representation is generally quite small (calculation reveals the all-time mean to be 0.026), so it is plausible that this was merely a minor theme that increased in importance.
</p>
<br>
<sh> Potential Improvements</sh>
<ul>
<li>
This analysis could be adapted to Wikipedias of other languages &mdash; Japanese, German, Russian, Spanish, etc.
This would allow one to track and compare particular changes in sentiment over time, across several language groups, i.e. across cultural groups,
which would potentially answer interesting questions (e.g. &ldquo;to what extents have different societies become more sexually liberal in the last fifty years?&rdquo;),
and might yield quantitative indications as to the extent to which cultures have become homogenized in recent decades.
</li>
<br>
<li>Beyond just removing punctuation and stopwords, it would be useful to have the option to remove names from the <i>Frequencies</i> dataset, or to track the proportional prevalence of names in episode summaries over time (which could signal if there is some change in how character- or action-driven the episodes are). This would be easy to implement, and just requires an extensive list of names.</li>
<br>
<li>It would be really nice to have an easily-usable JavaScript frontend for this, such that it can actually be used as easily as the Google Books ngram viewer.</li>
</ul>
<br>
<sh>Download</sh>
	<p>The GitHub repository is available <a href="https://github.com/Datamine/Television">here</a>. Please contact me if you find any bugs/errors, or use my work for any interesting derivative work &mdash; I'd like to see what you come up with. </p>

			<div class="notice">
				<br><br>This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
			</div>
		</div>
	</body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74324013-1', 'auto');
  ga('send', 'pageview');

</script>
</html>

